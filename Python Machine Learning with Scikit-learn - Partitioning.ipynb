{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning with Scikit-learn - Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we introduced a Python Machine Learning library (`scikit-learn`) built using Numpy, Scipy and matplotlib.\n",
    "\n",
    "The different modules can be found in the [documentation] (https://scikit-learn.org/stable/user_guide.html).\n",
    "\n",
    "As it is a heavy library, we do not normally import it in its entirety; instead, **we import only the necessary functionality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "For each of the following hypotheses, **identify whether it is a possible source of *bias* or variance**:\n",
    "- The use of **very flexible** models (e.g., non-parametric, non-linear, or with many parameters).\n",
    "    - [] Bias.\n",
    "    - [X] Variance.\n",
    "- The use of **models with simplistic *assumptions* on the data.**\n",
    "    - [X] Bias.\n",
    "    - [] Variance.\n",
    "- **Ignore important features**.\n",
    "    - [X] Bias.\n",
    "    - [] Variance.\n",
    "- Has **more features than training examples**.\n",
    "    - [] Bias.\n",
    "    - [X] Variance.\n",
    "\n",
    "**How can we evaluate** the *bias* and variance of a model?\n",
    "\n",
    "** Through the Train-test Split process, that is, the definition and comparison of training errors vs test errors, seeking to optimize them.\n",
    "\n",
    "# Exercise 2\n",
    "\n",
    "Overall, data partitioning is done with the aim of **measuring the predictive performance of a model**, i.e., generalization.\n",
    "\n",
    "To measure the performance of the model we can test it on a set of test data that was not used for training.\n",
    "\n",
    "This technique is known as *hold-out set*.\n",
    "\n",
    "**Exercise**: For this, we will test to create two datasets, at random, **training** (70% of the data) and **test** (30% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### BEGIN SOLUTION\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape == (354, 13)\n",
    "assert X_test.shape == (152, 13)\n",
    "assert y_train.shape == (354,)\n",
    "assert y_test.shape == (152,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "As we have seen, however, the training dataset, so that you can adequately estimate the generalization error, **should not be burned**.\n",
    "\n",
    "What we mean by this is that if we continually evaluate models according to the performance under test and choose the best ones, we will:\n",
    "- **Inflating performance** under test (because we are optimizing for that)\n",
    "- Losing the possibility of using it to predict the **performance of the model in data never seen before** (*out-of-sample*).\n",
    "\n",
    "However, before choosing a \"final\" model it is essential to choose from several alternative models.\n",
    "\n",
    "In this sense, **we broke the training dataset into training and validation**, and used the second one to choose the best model (s).\n",
    "\n",
    "**Exercise**: Now, we want to create three datasets: **training** (80% of data), **validation** (10% of data) and **test** (10% of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### BEGIN SOLUTION\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=.1/.9)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape == (404, 13)\n",
    "assert X_val.shape == X_test.shape == (51, 13)\n",
    "assert y_train.shape == (404,)\n",
    "assert y_val.shape == y_test.shape == (51,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Sometimes, however, **it is not possible to give up a significant part of the data** to create training and validation datasets.\n",
    "\n",
    "A more efficient alternative is to use *cross-validation*. There are **several types of cross-validation**:\n",
    "- *Leave-one-out*.\n",
    "    - Train $n$ models using $n-1$ observations and, for each of them, calculate the error for the $n$-th observation, omitted in training.\n",
    "    - Calculate the average of the errors.\n",
    "- *Leave-k-out*.\n",
    "    - Similar to *leave-one-out* but with *k* observations omitted from training (for validation) in each iteration.\n",
    "- *K-fold*.\n",
    "    - The dataset is partitioned into *k* sub-datasets and each one is omitted (for validation) in each iteration.\n",
    "\n",
    "The last alternative, *leave-k-out*, is, in practice, more consistent and **more robust to small changes in the data**.\n",
    "\n",
    "This is the most used alternative, in practice, to **evaluate and select models** of machine learning.\n",
    "\n",
    "**Exercise**: Evaluate the models below using *K-fold* cross-validation, with 4 *folds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Linear Regression</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.783102</td>\n",
       "      <td>0.722671</td>\n",
       "      <td>0.860863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761287</td>\n",
       "      <td>0.698294</td>\n",
       "      <td>0.793908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.864839</td>\n",
       "      <td>0.775026</td>\n",
       "      <td>0.896901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.434137</td>\n",
       "      <td>0.575477</td>\n",
       "      <td>0.687971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decision Tree  Linear Regression  Random Forest\n",
       "0       0.783102           0.722671       0.860863\n",
       "1       0.761287           0.698294       0.793908\n",
       "2       0.864839           0.775026       0.896901\n",
       "3       0.434137           0.575477       0.687971"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some models for testing.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "lr = LinearRegression()\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "dt_results = cross_val_score(dt, X_train, y_train, cv=4)\n",
    "lr_results = cross_val_score(lr, X_train, y_train, cv=4)\n",
    "rf_results = cross_val_score(rf, X_train, y_train, cv=4)\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Decision Tree': dt_results,\n",
    "    'Linear Regression': lr_results,\n",
    "    'Random Forest': rf_results\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we **interpret the results**? Include in the discussion:\n",
    "- What each column means.\n",
    "\n",
    "*** Each of the columns refers to the 3 models under analysis.\n",
    "\n",
    "- What each line means.\n",
    "\n",
    "*** Each of the lines refers to the 4 experiments made on the 4 folds generated by the K-fold partitioning process.\n",
    "\n",
    "- How to interpret the values in the cells.\n",
    "     - Each result is a measure of the model's performance (not error).\n",
    "     - The higher the result, the better the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Finally, we calculate the mean and the standard deviation (as a measure of variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest        0.809911\n",
       "Decision Tree        0.710841\n",
       "Linear Regression    0.692867\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "models_mean = results.mean().sort_values(ascending=False)\n",
    "models_mean\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear Regression    0.084555\n",
       "Random Forest        0.091813\n",
       "Decision Tree        0.189778\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "models_std = results.std().sort_values()\n",
    "models_std\n",
    "\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decision Tree        0.036016\n",
       "Linear Regression    0.007149\n",
       "Random Forest        0.008430\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_var = results.var()\n",
    "models_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "Based on the results above, **sort the models in decreasing order of bias**.\n",
    " \n",
    " - Decision Tree, Linear Regression, Random Forest\n",
    "\n",
    "## Exercise 5.2\n",
    "\n",
    "Based on the results above, **sort the models in descending order of variance**.\n",
    "\n",
    " - Decision Tree, Linear Regression, Random Forest\n",
    "\n",
    "## Exercise 5.3\n",
    "\n",
    "Do these results seem to **contradict**, in any way, the *bias-variance trade-off*? If so, **why**?\n",
    "\n",
    " - Although it seems to be so, in fact what is happening is that the Random Forest model is at the optimal level of complexity, with reduced Variance and reduced Bias, the sweet spot. This fact contradicts the loss-gain ratio that the bias-variance trade-off represents (losing Bias gains variance and vice versa).\n",
    " \n",
    " - In addition, looking at the table of values of the models, it appears that the Decision Tree model presents high flexibility in its analysis, sometimes having very high values and other times very low values.\n",
    " \n",
    " - The Linear Regression model presents, in a more consistent way, low performance values, indicating high levels of Bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
